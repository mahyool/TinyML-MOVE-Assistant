# ðŸŽ§ TinyML MOVE Assistant â€” Audio-Based Gesture Recognition

This project is based on Google's MOVE experiment and the accompanying GitHub repo by Yeonhee77. It demonstrates how machine learning models can recognize motion gestures like clapping, stomping, and snapping based purely on sound â€” and run directly on edge devices using TinyML.

## ðŸ§  What's Included

- Pretrained model archive: `TalkFile_TinySpeechTrainer-models-20231102.tgz`
- References to the original project and online experiment
- Documentation for potential extensions and how I plan to customize the system

## ðŸ”— Key Resources

- ðŸ”¬ MOVE Experiment by Google: https://experiments.withgoogle.com/move  
- ðŸ§ª GitHub Repo by Yeonhee77: https://github.com/Yeonhee77/MOVE

## ðŸ’¡ My Contribution & Plan

While this version uses the original pretrained models, I plan to:

1. Collect my own custom audio samples (clap, knock, snap)
2. Retrain the models using TensorFlow or Edge Impulse
3. Deploy to Arduino Nano 33 BLE Sense and integrate gesture recognition into physical interactions

Stay tuned for updates!

---

ðŸ“¬ Feel free to contact me
